name: üîÑ ETL Pipeline (S3 -> RDS)

on:
  schedule:
    - cron: '0 2 * * *'  # Agendado para 2:00 AM UTC diariamente
  workflow_dispatch:      # Gatilho manual para testes

jobs:
  etl_process:
    name: üè≠ Run ETL Data Pipeline
    runs-on: ubuntu-latest
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4

      - name: üêç Setup Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: üì§ Upload Data JSON to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: us-east-1
        run: |
          # Define environment
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            BUCKET_NAME="nexus-sus-data-lake-prod"
          else
            BUCKET_NAME="nexus-sus-data-lake-staging"
          fi
          
          # Upload arquivo JSON de dados
          echo "üì§ Uploading data JSON to S3..."
          aws s3 cp data/mgdi_ms_3f6.json s3://$BUCKET_NAME/dados_brutos.json
          
          echo "‚úÖ Data uploaded to s3://$BUCKET_NAME/dados_brutos.json"

      - name: üì¶ Create Lambda Deployment Package
        run: |
          mkdir -p package
          
          # 1. Instalar depend√™ncias SEM pandas/numpy (leve!)
          pip install \
            --platform manylinux2014_x86_64 \
            --target ./package \
            --implementation cp \
            --python-version 3.11 \
            --only-binary=:all: --upgrade \
            sqlalchemy psycopg2-binary

          # 2. Copiar script ETL e SQL Schema
          cp nexus-sus-etl/extrair_sus.py ./package/
          cp nexus-sus-etl/create_table.sql ./package/
          
          # 3. Limpar lixo
          find package -name "*.dist-info" -type d -exec rm -rf {} + 2>/dev/null || true
          find package -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
          
          # 4. Zipar tudo
          cd package
          zip -r ../deployment_package.zip .
          cd ..
          
          echo "üì¶ Pacote ZIP criado:"
          du -h deployment_package.zip

      - name: üöÄ Deploy & Invoke Lambda
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: us-east-1
        run: |
          # Define Environment
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            ENV="prod"
          else
            ENV="staging"
          fi
          
          LAMBDA_NAME="nexus-sus-etl-${ENV}"
          BUCKET_NAME="nexus-sus-data-lake-${ENV}"
          S3_KEY="artifacts/lambda/etl-${{ github.sha }}.zip"
          
          echo "üöÄ Config: Lambda=$LAMBDA_NAME | Bucket=$BUCKET_NAME"
          
          # 1. Upload para S3
          echo "üì§ Uploading package to s3://$BUCKET_NAME/$S3_KEY"
          aws s3 cp deployment_package.zip s3://$BUCKET_NAME/$S3_KEY
          
          # 2. Update Lambda Code via S3
          echo "üîÑ Updating Function Code..."
          aws lambda update-function-code \
            --function-name $LAMBDA_NAME \
            --s3-bucket $BUCKET_NAME \
            --s3-key $S3_KEY

          echo "‚è≥ Waiting for update..."
          sleep 15
          
          echo "‚ö° Invoking Lambda..."
          aws lambda invoke \
            --function-name $LAMBDA_NAME \
            --cli-binary-format raw-in-base64-out \
            --payload '{}' \
            response.json
            
          echo "üìÑ Lambda Output:"
          cat response.json
          
          if grep -q "errorMessage" response.json; then
            echo "‚ùå ETL Falhou!"
            exit 1
          fi

      - name: ‚úÖ Notify Success
        if: success()
        run: echo "ETL (Lambda) Conclu√≠do com Sucesso!"

      - name: ‚ùå Notify Failure
        if: failure()
        run: echo "ETL Falhou. Verifique os logs da Lambda no CloudWatch."
